{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d36d7e81",
   "metadata": {},
   "source": [
    "\n",
    "# Exploring the SHAP library\n",
    "\n",
    "## Loading the dataset and defining the predictive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b8ace7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d84434",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "survey = fetch_openml(data_id=534, as_frame=True)\n",
    "survey.frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3354d72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    survey.data, survey.target, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8cddce",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.compose import make_column_selector, make_column_transformer\n",
    "\n",
    "categorical_columns = make_column_selector(dtype_include=\"category\")\n",
    "numerical_columns = make_column_selector(dtype_exclude=\"category\")\n",
    "preprocessor = make_column_transformer(\n",
    "    (\n",
    "        OrdinalEncoder(\n",
    "            handle_unknown=\"use_encoded_value\",\n",
    "            unknown_value=-1,\n",
    "        ),\n",
    "        categorical_columns,\n",
    "    ),\n",
    "    remainder=\"passthrough\",\n",
    "    verbose_feature_names_out=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fa6838",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "model = make_pipeline(\n",
    "    preprocessor,\n",
    "    HistGradientBoostingRegressor(max_iter=10_000, early_stopping=True, random_state=0),\n",
    ")\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5bc4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print(\n",
    "    f\"MAE on the training set: \"\n",
    "    f\"{mean_absolute_error(y_train, model.predict(X_train)):.3f} $/hour\"\n",
    ")\n",
    "print(\n",
    "    f\"MAE on the testing set: \"\n",
    "    f\"{mean_absolute_error(y_test, model.predict(X_test)):.3f} $/hour\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d319a9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(y_train, bins=30, density=True)\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xlabel(\"$/hour\")\n",
    "_ = plt.title(\"Target distribution \\nin the training set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2661ff99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "print(\n",
    "    f\"MAPE on the training set: \"\n",
    "    f\"{mean_absolute_percentage_error(y_train, model.predict(X_train)) * 100:.1f}%\"\n",
    ")\n",
    "print(\n",
    "    f\"MAPE on the testing set: \"\n",
    "    f\"{mean_absolute_percentage_error(y_test, model.predict(X_test)) * 100:.1f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c40345",
   "metadata": {},
   "source": [
    "\n",
    "## What SHAP values mean?\n",
    "\n",
    "![shap_1](../images/shap_1.png)\n",
    "![shap_2](../images/shap_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f796862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "feature_names = categorical_columns(X_train) + numerical_columns(X_train)\n",
    "X_train_preprocessed = pd.DataFrame(\n",
    "    preprocessor.fit_transform(X_train), columns=feature_names\n",
    ")\n",
    "X_test_preprocessed = pd.DataFrame(\n",
    "    preprocessor.transform(X_test), columns=feature_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635f199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "explainer = shap.Explainer(\n",
    "    model[-1], masker=X_train_preprocessed, feature_perturbation=\"interventional\"\n",
    ")\n",
    "shap_values = explainer(X_test_preprocessed)\n",
    "shap_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25680c5",
   "metadata": {},
   "source": [
    "\n",
    "## Explain model's prediction vs. mean prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09558fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X_test.iloc[[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e91033",
   "metadata": {},
   "source": [
    "\n",
    "The reported SHAP values for the different features are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebb34f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(shap_values[0].values, index=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821ca063",
   "metadata": {},
   "source": [
    "\n",
    "Taking into account the base value, then the model prediction corresponds to\n",
    "the following sum:\n",
    "\n",
    "shap_values[0].values.sum() + shap_values.base_values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d67af4",
   "metadata": {},
   "source": [
    "\n",
    "## SHAP as a visual debugging tool\n",
    "\n",
    "SHAP package comes with handy plotting facilities to visualize the Shapley\n",
    "values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c73ccf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.waterfall(shap_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6f9c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "shap.plots.force(\n",
    "    shap_values.base_values[0],\n",
    "    shap_values.values[0],\n",
    "    feature_names=feature_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457a81ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a48b77",
   "metadata": {},
   "source": [
    "\n",
    "## Global explanation by averaging local explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bded1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "importances = permutation_importance(model, X_test, y_test, n_jobs=-1)\n",
    "sorted_idx = importances.importances_mean.argsort()\n",
    "\n",
    "importances = pd.DataFrame(\n",
    "    importances.importances[sorted_idx].T, columns=X_test.columns[sorted_idx]\n",
    ")\n",
    "importances.plot.box(vert=False, whis=100, figsize=(8, 6))\n",
    "plt.axvline(0, color=\"k\", linestyle=\"--\")\n",
    "plt.xlabel(\"Decrease in R2 score\")\n",
    "_ = plt.title(\"Permutation importances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35710b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec3ba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "abs_shap_values = pd.DataFrame(\n",
    "    np.abs(shap_values.values),\n",
    "    columns=X_train_preprocessed.columns,\n",
    ")\n",
    "abs_shap_values[abs_shap_values.mean().sort_values().index].plot.box(\n",
    "    vert=False, whis=100, figsize=(8, 6)\n",
    ")\n",
    "_ = plt.xlabel(\"|SHAP value|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881918de",
   "metadata": {},
   "source": [
    "\n",
    "## Be aware of some pitfalls regarding SHAP\n",
    "\n",
    "Pitfalls can come from:\n",
    "\n",
    "- some issues with the usage of the library\n",
    "- some limitations due to theoretical assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b535a119",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(\n",
    "    model[-1],\n",
    "    masker=X_train_preprocessed,\n",
    ")\n",
    "shap_values = explainer(X_test_preprocessed)\n",
    "explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4688e47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.feature_perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70f98ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(model[-1])\n",
    "_ = explainer(X_test_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf46a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.feature_perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d034c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(model[-1], feature_perturbation=\"interventional\")\n",
    "_ = explainer(X_test_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c09369d",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.feature_perturbation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7df2d2",
   "metadata": {},
   "source": [
    "\n",
    "![book_model](../images/feature_perturbation.png)\n",
    "![book_model](../images/causal_problem.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cfcbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate(\n",
    "    [[[0, 0]] * 400, [[0, 1]] * 100, [[1, 0]] * 100, [[1, 1]] * 400], axis=0\n",
    ")\n",
    "X\n",
    "\n",
    "y = np.array([0] * 400 + [50] * 100 + [50] * 100 + [100] * 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96405678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "\n",
    "tree_1 = DecisionTreeRegressor(random_state=0).fit(X, y)\n",
    "plt.figure(figsize=(10, 6))\n",
    "_ = plot_tree(tree_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e28144",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_2 = DecisionTreeRegressor(random_state=4).fit(X, y)\n",
    "plt.figure(figsize=(10, 6))\n",
    "_ = plot_tree(tree_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510a0bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array([[1, 1]])\n",
    "explainer = shap.explainers.Exact(tree_1.predict, X)\n",
    "explainer(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d590ebe",
   "metadata": {},
   "source": [
    "\n",
    "Let's disable the internal subsampling to compute the expected value on the\n",
    "full training set and therefore the true Shapeley values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b53d806",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.explainers.Exact(\n",
    "    tree_1.predict, masker=shap.maskers.Independent(X, max_samples=X.shape[0])\n",
    ")\n",
    "explainer(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8c29da",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.explainers.Exact(\n",
    "    tree_2.predict, masker=shap.maskers.Independent(X, max_samples=X.shape[0])\n",
    ")\n",
    "explainer(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d6285f",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(tree_1)\n",
    "explainer(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc34e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(tree_2)\n",
    "explainer(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb474c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc8eed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.feature_perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2070262",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(tree_1, shap.maskers.Independent(X, max_samples=X.shape[0]))\n",
    "explainer(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20750ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(tree_2, shap.maskers.Independent(X, max_samples=X.shape[0]))\n",
    "explainer(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ba4580",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1288669c",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.feature_perturbation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3b7f16",
   "metadata": {},
   "source": [
    "\n",
    "### References\n",
    "\n",
    "[1] Kumar, I. Elizabeth, et al. \"Problems with Shapley-value-based\n",
    "explanations as feature importance measures.\" International Conference on\n",
    "Machine Learning. PMLR, 2020.\n",
    "\n",
    "[2] Janzing, Dominik, Lenon Minorics, and Patrick Blöbaum. \"Feature relevance\n",
    "quantification in explainable AI: A causal problem.\" International Conference\n",
    "on artificial intelligence and statistics. PMLR, 2020."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
